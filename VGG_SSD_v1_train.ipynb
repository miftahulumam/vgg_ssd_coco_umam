{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23bc0f03",
   "metadata": {},
   "source": [
    "# Load The Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57ddd018",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20844\\4123772955.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPIL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./model/\")\n",
    "\n",
    "from utils import *\n",
    "from model.mutibox_loss import MultiBoxLoss\n",
    "from model.metrics.metric import Metrics\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "import PIL\n",
    "import os\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.transforms import ToTensor, Compose, Resize, Normalize, Lambda\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8fb732",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62849c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d9cdac",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258aa70b",
   "metadata": {},
   "source": [
    "## Build The Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class backbone_vgg16(nn.Module):\n",
    "    def __init__(self, fine_tune = True):\n",
    "        super(backbone_vgg16_module, self).__init__()\n",
    "        backbone = torchvision.models.vgg16(weights=\"IMAGENET1K_V1\")\n",
    "        \n",
    "        if fine_tune == False\n",
    "            for param in backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        feature_maps = list(backbone.children())[0]\n",
    "        \n",
    "        self.feature_1 = nn.Sequential(*feature_maps[:23])\n",
    "        self.feature_2 = nn.Sequential(*feature_maps[23:])\n",
    "        self.conv_6 = nn.Conv2d(512, 1024, kernel_size= (3, 3), padding= 6, dilation= 6)\n",
    "        self.conv_7 = nn.Conv2d(1024, 1024, kernel_size= (1, 1))\n",
    "        \n",
    "    def forward(self, image):\n",
    "        x = image\n",
    "        x = self.feature_1(x)\n",
    "        out_1 = x\n",
    "        \n",
    "        x = self.feature_2(x)\n",
    "        x = F.relu(self.conv_6(x))\n",
    "        out_2 = F.relu(self.conv_7(x))\n",
    "        \n",
    "        return out_1, out_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9924907f",
   "metadata": {},
   "source": [
    "## Build the Neck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eeb7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class extra_feature_layers(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(extra_feature_layers, self).__init__()\n",
    "        \n",
    "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=(1, 1), padding= 0)\n",
    "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size= (3, 3), padding= 1, stride= 2)\n",
    "        \n",
    "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size= (1, 1), padding= 0)\n",
    "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size= (3, 3), padding= 1, stride= 2)\n",
    "        \n",
    "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size= (1, 1), padding= 0)\n",
    "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size= (3, 3), padding= 0)\n",
    "        \n",
    "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size= (1, 1), padding= 0)\n",
    "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size= (3, 3), padding= 0)\n",
    "        \n",
    "        self.weights_init()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv8_1(x))                                #(N, 256, 19, 19)\n",
    "        x = F.relu(self.conv8_2(x))                                #(N, 512, 10, 10)\n",
    "        conv8_2_out = x\n",
    "        \n",
    "        x = F.relu(self.conv9_1(x))                                #(N, 128, 10, 10)\n",
    "        x = F.relu(self.conv9_2(x))                                #(N, 256, 5, 5)\n",
    "        conv9_2_out = x\n",
    "        \n",
    "        x = F.relu(self.conv10_1(x))                               #(N, 128, 5, 5)\n",
    "        x = F.relu(self.conv10_2(x))                               #(N, 256, 3, 3)\n",
    "        conv10_2_out = x\n",
    "        \n",
    "        x = F.relu(self.conv11_1(x))                               #(N, 128, 3, 3)\n",
    "        conv11_2_out = F.relu(self.conv11_2(x))                    #(N, 256, 1, 1)\n",
    "        \n",
    "        return conv8_2_out, conv9_2_out, conv10_2_out, conv11_2_out\n",
    "    \n",
    "    def weights_init(self):\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.zeros_(c.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54a7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2_Norm(nn.Module):\n",
    "    def __init__(self, channels, scale):\n",
    "        super(L2Norm, self).__init__()\n",
    "        \n",
    "        self.channels = channels\n",
    "        self.scale = scale\n",
    "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, channels, 1, 1))\n",
    "        \n",
    "        self.reset_params()\n",
    "        \n",
    "    def reset_params(self):\n",
    "        nn.init.constant_(self.rescale_factors, self.scale)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()\n",
    "        x = x / norm\n",
    "        out = x * self.rescale_factors\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d6894",
   "metadata": {},
   "source": [
    "## Build The Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e652c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_layer(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(detection_layer, self).__init__()\n",
    "        \n",
    "        num_prior_boxes = {\"backbone_1\": 4, \"backbone_2\":6, \"conv8_2\":6, \"conv9_2\":6, \"conv10_2\":4, \"conv11_2\":4}\n",
    "        \n",
    "        # localizer heads\n",
    "        self.backbone_1_loc = nn.Conv2d(512, num_prior_boxes[\"backbone_1\"]*4,  kernel_size= (3, 3), padding= 1)\n",
    "        self.backbone_2_loc = nn.Conv2d(1024, num_prior_boxes[\"backbone_2\"]*4,  kernel_size= (3, 3), padding= 1)\n",
    "        self.conv8_2_loc = nn.Conv2d(512, num_prior_boxes[\"conv8_2\"]*4,  kernel_size= (3, 3), padding= 1)\n",
    "        self.conv9_2_loc = nn.Conv2d(256, num_prior_boxes[\"conv9_2\"]*4,  kernel_size= (3, 3), padding= 1)\n",
    "        self.conv10_2_loc = nn.Conv2d(1024, num_prior_boxes[\"conv10_2\"]*4,  kernel_size= (3, 3), padding= 1)\n",
    "        self.conv11_2_loc = nn.Conv2d(1024, num_prior_boxes[\"conv11_2\"]*4,  kernel_size= (3, 3), padding= 1)\n",
    "        \n",
    "        # classifier heads\n",
    "        self.backbone_1_cls = nn.Conv2d(512, num_prior_boxes[\"backbone_1\"]*n_classes,  kernel_size= (3, 3), padding= 1)\n",
    "        self.backbone_2_cls = nn.Conv2d(1024, num_prior_boxes[\"backbone_2\"]*n_classes,  kernel_size= (3, 3), padding= 1)\n",
    "        self.conv8_2_cls = nn.Conv2d(512, num_prior_boxes[\"conv8_2\"]*n_classes,  kernel_size= (3, 3), padding= 1)\n",
    "        self.conv9_2_cls = nn.Conv2d(256, num_prior_boxes[\"conv9_2\"]*n_classes,  kernel_size= (3, 3), padding= 1)\n",
    "        self.conv10_2_cls = nn.Conv2d(1024, num_prior_boxes[\"conv10_2\"]*n_classes,  kernel_size= (3, 3), padding= 1)\n",
    "        self.conv11_2_cls = nn.Conv2d(1024, num_prior_boxes[\"conv11_2\"]*n_classes,  kernel_size= (3, 3), padding= 1)\n",
    "        \n",
    "        self.weights_init()\n",
    "    \n",
    "    def forward(self, backbone_1_out, backbone_2_out, conv8_2_out, conv9_2_out,\n",
    "                conv10_2_out, conv11_2_out):\n",
    "\n",
    "        batch_size = backbone_1_out.size(0)\n",
    "        \n",
    "        ### Bounding box prediction\n",
    "        loc_backbonet_1 = self.backbone_1_loc(backbone_1_out)                        #(N, 16, 38, 38)\n",
    "        loc_backbone_1 = loc_backbone_1.permute(0, 2, 3, 1).contiguous()            #(N, 38, 38, 16)\n",
    "        loc_backbone_1 = loc_backbone_1.view(batch_size, -1, 4)                     #(N, 5776, 4)\n",
    "        assert loc_backbone_1.size(1) == 5776\n",
    "        \n",
    "        loc_backbone_2 = self.backbone2_loc(backbone_2_out)                                 #(N, 24, 19, 19)\n",
    "        loc_backbone_2 = loc_backbone_2.permute(0, 2, 3, 1).contiguous()                #(N, 19, 19, 24)\n",
    "        loc_backbone_2 = loc_backbone_2.view(batch_size, -1, 4)                         #(N, 2166, 4)\n",
    "        assert loc_backbone_2.size(1) == 2166\n",
    "        \n",
    "        loc_conv8_2 = self.conv8_2_loc(conv8_2_out)                           #(N, 24, 10, 10)\n",
    "        loc_conv8_2 = loc_conv8_2.permute(0, 2, 3, 1).contiguous()            #(N, 10, 10, 24)\n",
    "        loc_conv8_2 = loc_conv8_2.view(batch_size, -1, 4)                     #(N, 600, 4)\n",
    "        assert loc_conv8_2.size(1) == 600\n",
    "        \n",
    "        loc_conv9_2= self.conv9_2_loc(conv9_2_out)                            #(N, 24, 5, 5)\n",
    "        loc_conv9_2 = loc_conv9_2.permute(0, 2, 3, 1).contiguous()            #(N, 5, 5, 24)\n",
    "        loc_conv9_2 = loc_conv9_2.view(batch_size, -1, 4)                     #(N, 150, 4)\n",
    "        assert loc_conv9_2.size(1) == 150\n",
    "        \n",
    "        loc_conv10_2 = self.conv10_2_loc(conv10_2_out)                      #(N, 16, 3, 3)\n",
    "        loc_conv10_2 = loc_conv10_2.permute(0, 2, 3, 1).contiguous()          #(N, 3, 3, 16)\n",
    "        loc_conv10_2 = loc_conv10_2.view(batch_size, -1, 4)                   #(N, 36, 4)\n",
    "        assert loc_conv10_2.size(1) == 36\n",
    "        \n",
    "        loc_conv11_2 = self.conv11_2_loc(conv11_2_out)                      #(N, 16, 1, 1)\n",
    "        loc_conv11_2 = loc_conv11_2.permute(0, 2, 3, 1).contiguous()          #(N, 1, 1, 16)\n",
    "        loc_conv11_2 = loc_conv11_2.view(batch_size, -1, 4)                   #(N, 4, 4)\n",
    "        assert loc_conv11_2.size(1) == 4\n",
    "\n",
    "        ### Classifiers\n",
    "        cls_backbone_1 = self.backbone_1_cls(backbone_1_out)                         #(N, 4*classes, 38, 38)\n",
    "        cls_backbone_1 = cls_backbone_1.permute(0, 2, 3, 1).contiguous()            #(N, 38, 38, 4*classes)\n",
    "        cls_backbone_1 = cls_backbone_1.view(batch_size, -1, self.n_classes)      #(N, 5776, classes )\n",
    "        assert cls_backbone_1.size(1) == 5776\n",
    "        \n",
    "        cls_backbone_2 = self.backbone_2_cls(backbone_2_out)                               #(N, 6*classes, 19, 19)\n",
    "        cls_backbone_2 = cls_backbone_2.permute(0, 2, 3, 1).contiguous()                #(N, 19, 19, 6*classes)\n",
    "        cls_backbone_2 = cls_backbone_2.view(batch_size, -1, self.n_classes)          #(N, 2166, classes)\n",
    "        assert cls_backbone_2.size(1) == 2166        \n",
    "        \n",
    "        cls_conv8_2 = self.conv8_2_cls(conv8_2_out)                         #(N, 6*clases, 10, 10)\n",
    "        cls_conv8_2 = cls_conv8_2.permute(0, 2, 3, 1).contiguous()            #(N, 10, 10, 6*classes)\n",
    "        cls_conv8_2 = cls_conv8_2.view(batch_size, -1, self.n_classes)      #(N, 600, classes)\n",
    "        assert cls_conv8_2.size(1) == 600\n",
    "        \n",
    "        cls_conv9_2 = self.conv9_2_cls(conv9_2_out)                         #(N, 6*classes, 5, 5)\n",
    "        cls_conv9_2 = cls_conv9_2.permute(0, 2, 3, 1).contiguous()            #(N, 5, 5, 6*classes)\n",
    "        cls_conv9_2 = cls_conv9_2.view(batch_size, -1, self.n_classes)      #(N, 150, classes)\n",
    "        assert cls_conv9_2.size(1) == 150\n",
    "        \n",
    "        cls_conv10_2 = self.conv10_2_cls(conv10_2_out)                      #(N, 4*classes, 3, 3)\n",
    "        cls_conv10_2 = cls_conv10_2.permute(0, 2, 3, 1).contiguous()          #(N, 3, 3, 4*classes)\n",
    "        cls_conv10_2 = cls_conv10_2.view(batch_size, -1, self.n_classes)    #(N, 36, classes)\n",
    "        assert cls_conv10_2.size(1) == 36\n",
    "        \n",
    "        cls_conv11_2 = self.conv11_2_cls(conv11_2_out)                      #(N, 4*classes, 1, 1)\n",
    "        cls_conv11_2 = cls_conv11_2.permute(0, 2, 3, 1).contiguous()          #(N, 1, 1, 4*classes)\n",
    "        cls_conv11_2 = cls_conv11_2.view(batch_size, -1, self.n_classes)    #(N, 4, classes)\n",
    "        assert cls_conv11_2.size(1) == 4\n",
    "        \n",
    "        ### All predictions\n",
    "        locs_pred = torch.cat([loc_backbone_out_1, loc_backbone_out_2, loc_conv8_2, loc_conv9_2,\n",
    "                               loc_conv10_2, loc_conv11_2], dim= 1)    #(N, 8732, 4)\n",
    "        assert locs_pred.size(0) == batch_size\n",
    "        assert locs_pred.size(1) == 8732\n",
    "        assert locs_pred.size(2) == 4\n",
    "    \n",
    "        cls_pred = torch.cat([cls_backbone_out_1, cls_backbone_out_2, cls_conv8_2, cls_conv9_2,\n",
    "                              cls_conv10_2, cls_conv11_2], dim= 1)    #(N, 8732, classes)\n",
    "        assert cls_pred.size(0) == batch_size\n",
    "        assert cls_pred.size(1) == 8732\n",
    "        assert cls_pred.size(2) == self.n_classes\n",
    "        \n",
    "        return locs_pred, cls_pred\n",
    "    \n",
    "    def weights_init(self):\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.zeros_(c.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a59a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSD(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SSD, self).__init__()\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.backbone = backbone_vgg16(fine_tune = True)\n",
    "        self.neck = extra_feature_layers()\n",
    "        self.head = detection_layer(n_classes)\n",
    "        \n",
    "        #Rescale factor for conv4_3, it is learned during back-prop\n",
    "        self.L2_Norm = L2_Norm(channels= 512, scale= 20)\n",
    "        \n",
    "        #Prior boxes coordinate cx, cy, w, h\n",
    "        self.default_boxes = self.create_default_boxes()\n",
    "        \n",
    "    def forward(self, image):\n",
    "        backbone_out_1, backbone_out_2 = self.backbone(image)\n",
    "        \n",
    "        backbone_out_1_norm = self.L2_Norm(backbone_out_1)\n",
    "        \n",
    "        neck_out_1, neck_out_2, neck_out_3, neck_out_4 = self.neck(backbone_out_2)\n",
    "        \n",
    "        locs_pred, cls_pred = self.head(backbone_out_1, backbone_out_2, \n",
    "                                        neck_out_1, neck_out_2, neck_out_3, neck_out_4)    #(N, 8732, 4) #(N, 8732, classes)\n",
    "        \n",
    "        return locs_pred, cls_pred\n",
    "    \n",
    "    def create_default_boxes(self):\n",
    "            fmap_wh = {\"conv4_3\": 38, \"conv7\": 19, \"conv8_2\": 10, \"conv9_2\": 5,\n",
    "                       \"conv10_2\": 3, \"conv11_2\": 1}\n",
    "\n",
    "            scales = {\"conv4_3\": 0.1, \"conv7\": 0.2, \"conv8_2\": 0.375,\n",
    "                      \"conv9_2\": 0.55, \"conv10_2\": 0.725, \"conv11_2\": 0.9}\n",
    "\n",
    "            aspect_ratios= {\"conv4_3\": [1., 2., 0.5], \"conv7\": [1., 2., 3., 0.5, 0.3333],\n",
    "                            \"conv8_2\": [1., 2., 3., 0.5, 0.3333], \n",
    "                            \"conv9_2\": [1., 2., 3., 0.5, 0.3333],\n",
    "                            \"conv10_2\": [1., 2., 0.5], \"conv11_2\": [1., 2., 0.5]}\n",
    "\n",
    "            fmaps = list(fmap_wh.keys())\n",
    "\n",
    "            default_boxes = []\n",
    "            for k, fmap in enumerate(fmaps):\n",
    "                for i in range(fmap_wh[fmap]):\n",
    "                    for j in range(fmap_wh[fmap]):\n",
    "                        cx = (j + 0.5) / fmap_wh[fmap]\n",
    "                        cy = (i + 0.5) / fmap_wh[fmap]\n",
    "\n",
    "                        for ratio in aspect_ratios[fmap]:\n",
    "                            default_boxes.append([cx, cy, scales[fmap]* math.sqrt(ratio), \n",
    "                                                  scales[fmap]/math.sqrt(ratio)]) #(cx, cy, w, h)\n",
    "\n",
    "                            if ratio == 1:\n",
    "                                try:\n",
    "                                    add_scale = math.sqrt(scales[fmap]*scales[fmaps[k+1]])\n",
    "                                except IndexError:\n",
    "                                    #for the last feature map\n",
    "                                    add_scale = 1.\n",
    "                                default_boxes.append([cx, cy, add_scale, add_scale])\n",
    "\n",
    "            default_boxes = torch.FloatTensor(default_boxes).to(device) #(8732, 4)\n",
    "            default_boxes.clamp_(0, 1)\n",
    "            assert default_boxes.size(0) == 8732\n",
    "            assert default_boxes.size(1) == 4\n",
    "            return default_boxes\n",
    "        \n",
    "    def detect(self, locs_pred, cls_pred, min_score, max_overlap, top_k):\n",
    "\n",
    "        batch_size = locs_pred.size(0)    #N\n",
    "        n_default_boxes = self.default_boxes.size(0)    #8732\n",
    "        cls_pred = F.softmax(cls_pred, dim= 2)    #(N, 8732, n_classes)\n",
    "        assert n_default_boxes == locs_pred.size(1) == cls_pred.size(1)\n",
    "        \n",
    "        all_images_boxes = []\n",
    "        all_images_labels = []\n",
    "        all_images_scores = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            #Decode object\n",
    "            decoded_locs = cxcy_to_xy(decode_bboxes(locs_pred[i], self.default_boxes)) #(8732, 4)\n",
    "            \n",
    "            image_boxes = []\n",
    "            image_labels = []\n",
    "            image_scores = []\n",
    "            \n",
    "            max_scores, best_label = cls_pred[i].max(dim= 1)    #(8732)\n",
    "            \n",
    "            #Check for each class\n",
    "            for c in range(1, self.num_classes):\n",
    "                class_scores = cls_pred[i][:, c]    #8732\n",
    "                score_above_min_score = class_scores > min_score\n",
    "                n_above_min_score = score_above_min_score.sum().item()\n",
    "                \n",
    "                if n_above_min_score == 0:\n",
    "                    continue\n",
    "                \n",
    "                class_scores = class_scores[score_above_min_score]    # <=8732\n",
    "                class_decoded_locs = decoded_locs[score_above_min_score] # <=8732\n",
    "                \n",
    "                #Sort pred boxes and socores by scores\n",
    "                class_scores, sort_id = class_scores.sort(dim= 0, descending= True)\n",
    "                class_decoded_locs = class_decoded_locs[sort_id]\n",
    "                \n",
    "                #Find overlap between pred locs\n",
    "                overlap = find_IoU(class_decoded_locs, class_decoded_locs)\n",
    "                \n",
    "                #Apply NMS\n",
    "                suppress = torch.zeros((n_above_min_score), dtype=torch.uint8).to(device)\n",
    "                \n",
    "                for box_id in range(class_decoded_locs.size(0)):\n",
    "                    if suppress[box_id] == 1:\n",
    "                        continue\n",
    "                    condition = overlap[box_id] > max_overlap\n",
    "                    condition = torch.tensor(condition, dtype=torch.uint8).to(device)\n",
    "                    suppress = torch.max(suppress, condition)\n",
    "                    \n",
    "                    suppress[box_id] = 0\n",
    "                \n",
    "                # Store only unsuppressed boxes for this class\n",
    "                image_boxes.append(class_decoded_locs[1 - suppress])\n",
    "                image_labels.append(torch.LongTensor((1 - suppress).sum().item() * [c]).to(device))\n",
    "                image_scores.append(class_scores[1 - suppress])\n",
    "            \n",
    "            if len(image_boxes) == 0:\n",
    "                image_boxes.append(torch.FloatTensor([[0., 0., 1., 1.]]).to(device))\n",
    "                image_labels.append(torch.LongTensor([0]).to(device))\n",
    "                image_scores.append(torch.FloatTensor([0.]).to(device))\n",
    "            \n",
    "            #Concat into single tensors\n",
    "            image_boxes = torch.cat(image_boxes, dim= 0)    #(n_objects, 4)\n",
    "            image_labels = torch.cat(image_labels, dim=0)  # (n_objects)\n",
    "            image_scores = torch.cat(image_scores, dim=0)  # (n_objects)\n",
    "            n_objects = image_scores.size(0)\n",
    "            \n",
    "            #Keep only the top k objects\n",
    "            if n_objects > top_k:\n",
    "                image_scores, sort_index = image_scores.sort(dim=0, descending=True)\n",
    "                image_scores = image_scores[:top_k]  # (top_k)\n",
    "                image_boxes = image_boxes[sort_index][:top_k]  # (top_k, 4)\n",
    "                image_labels = image_labels[sort_index][:top_k]  # (top_k)\n",
    "            \n",
    "            all_images_boxes.append(image_boxes)\n",
    "            all_images_labels.append(image_labels)\n",
    "            all_images_scores.append(image_scores)\n",
    "            \n",
    "        return all_images_boxes, all_images_labels, all_images_scores        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ee8100",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72a6b10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'coco.py'...\n",
      "remote: Not Found\n",
      "fatal: repository 'https://github.com/PythonAPI/pycocotools/coco.py/' not found\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/PythonAPI/pycocotools/coco.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f17d73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 400\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 101\n",
    "LEARNING_RATE = 0.001\n",
    "MOMENTUM = 0.9\n",
    "EARLY_STOP_THRESHOLD = 0.00001\n",
    "EARLY_STOP_PATIENCE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ccb5baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CocoDetection\n",
    "\n",
    "train_data_dir = 'coco-2017/train2017'\n",
    "train_ann_file = 'coco-2017/annotations_trainval2017/annotations/nstances_train2017.json'\n",
    "val_data_dir = 'coco-2017/val2017'\n",
    "val_ann_file = 'coco-2017/annotations/instances_val2017.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d117eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "\n",
    "transform = Compose([ToTensor(),\n",
    "                     Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "069bf02d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycocotools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12064\\2254874508.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCocoDetection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannFile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_ann_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCocoDetection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_data_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannFile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ann_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\coco.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, annFile, transform, target_transform, transforms)\u001b[0m\n\u001b[0;32m     32\u001b[0m     ) -> None:\n\u001b[0;32m     33\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mannFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycocotools'"
     ]
    }
   ],
   "source": [
    "train_dataset = CocoDetection(root=train_data_dir, annFile=train_ann_file, transform=transform)\n",
    "test_dataset = CocoDetection(root=val_data_dir, annFile=val_ann_file, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ac55264",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycocotools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12064\\166606527.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycocotools'"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a8a25",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f292a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
